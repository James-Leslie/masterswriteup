\chapter{Methodology}
This chapter details the methods used in this project. First, the aim of this masters dissertation is outlined against the context of the literature review. Next, the datasets used for training and validation of models are described and compared. Then, the ...

\section{Collaborative genre tagging}
Most of the previous work on collaborative filtering (CF) tackled the problem of predicting user preferences using their interaction history. The most successful methods all involve the use of latent factor models which are capable of learning otherwise hidden properties in the users and items in CF datasets. Recently this type of approach has been adapted from matrix factorisation models to neural architectures which capture latent factors in embedding layers, while allowing for non-linearity in successive hidden layers. 

CF models, by definition, do not make use of meta data for predicting user preferences -- this is the domain of content-based models. However, meta data such as item genres are still useful to users for assessing recommendations provided to them. Users cannot be expected to trust recommendations provided to them, without having some sort of description of the recommended item. While CF has shown its ability to provide recommendations to users without using content attributes, descriptive features are still useful for users to make informed decisions on \textit{which} recommendations to choose.

The following research questions motivated this minor dissertation: RQ1 -- can latent-factor-based CF be used to predict genres of items such as movies or books? RQ2 -- How does the choice of hyperparameters in the rating prediction model affect the performance of the genre prediction model?

\section{Datasets}
There were five different datasets used in this research project, all of which contain explicit ratings made by users across various item sets. Three datasets contain movie ratings, while the other two contain ratings of books and jokes respectively.

The MovieLens datasets are used frequently in CF research efforts. GroupLens research at the university of Minnesota is responsible for maintaining these data in four stable sets of different sizes: 100K, 1M, 10M and 20M. For the purposes of this research, the 20M was not necessary and so only the first three were used. Each set is named after the number of ratings it contains. The first two datasets use a 5 star, integer-only rating scale, while the 10M and 20M datasets each contain ratings from 0.5 - 5 stars, including half star ratings \parencite{harper2016movielens}.

The Jester dataset contains over 4.1 million ratings of 100 jokes from over 70 000 users. Ratings fall on a continuous scale between -10 and +10. The low number of items and the continuous ratings scale make this dataset notably different to the others used in this project \parencite{cf_1.2_eigentaste}.

The goodbooks-10k dataset takes its name after the fact that it contains 10 000 books, rated a total of just under 6 million times by over 50 000 users \parencite{goodbooks2017}.

\subsection{Number of users, items and ratings}
Each dataset has a different profile in terms of the ratios between numbers of users, items and ratings. The density is calculated as the number of ratings provided as a proportion of the total number of possible user-item interactions. ML 100K -- the dataset with the fewest ratings -- has 943 unique users and 1 682 unique movie titles. This would allow a total of 1 586 126 possible user-movie ratings. Since there are only 100 000 ratings in this dataset, its density is thus $\dfrac{100000}{1.586126} = 6.30\%$. Table \ref{tab:data-summary} provides a summary of these datasets. 

\begin{table}[H]
\label{tab:data-summary}
\centering
\begin{tabular}{c | c | c | c | c | c}
\toprule
\textbf{Name} & \textbf{Rating Scale} & \textbf{Users} & \textbf{Items} & \textbf{Ratings} & \textbf{Density} \\
\midrule
ML 100K & 1--5, stars & 943 & 1 682 & 100 000 & 6.30\% \\
ML 1M & 1--5, stars & 6 040 & 3 706 & 1 000 209 & 4.47\% \\
ML 10M & 0.5--5, half-stars & 69 878 & 10 681 & 10 000 054 & 1.34\% \\
Goodbooks-10k & 1--5, stars & 53 424 & 10 000 & 5 976 479 & 1.12\% \\
Jester & -10.0--10.0, decimal & 73 418 & 100 & 4 136 210 & 56.34\% \\
\bottomrule
\end{tabular}
\caption[Data summary]{Summary of the datasets used in this project}
\end{table}

The Jester dataset has the highest density by a significant margin (56.34\%). ML 100K, 1M and the goodbooks-10k sets all use an integer rating scale between 1 and 5 stars. The ML 10M set also uses a 5 star scale, but allows half star ratings, whereas the Jester dataset uses a continuous ratings scale between -10 and +10.

In terms of size, ML 100K is ten times smaller than the second smallest ML 1M set, while the largest set is ML 10M with 10 million ratings. The Jester data set contains the most users, while ML 10M contains the most items, albeit only by a slight margin over goodbooks-10k.

\subsection{Distribution of ratings}
Table \ref{tab:ratings-distribution} provides a summary of the distribution of ratings across users and items in each of the datasets.

All three MovieLens datasets have a minimum of 20 ratings per user, while some of the movies have only 1 rating.

The Jester dataset has the fewest average ratings per user; however, each item is rated over 41-thousand times on average, with even the least frequently rated item having over 18-thousand ratings.

The Goodbooks-10k dataset is similar to the MovieLens datasets in terms of its distribution across items; however, every book has at least 8 ratings as opposed to the 1 rating minimum of the MovieLens sets.

\begin{table}[H]
\label{tab:ratings-distribution}
\centering
\begin{tabular}{c | c | c | c | c | c | c}
\toprule
\textbf{Name} & \textbf{Min User} & \textbf{Max User} & \textbf{Avg User} & \textbf{Min Item} & \textbf{Max Item} & \textbf{Avg Item} \\
\midrule
ML 100K & 20 & 737 & 106 & 1 & 583 & 60 \\
ML 1M & 20 & 2 314 & 166 & 1 & 3 428 & 270 \\
ML 10M & 20 & 7 359 & 143 & 1 & 34 864 & 937 \\
Goodbooks-10k & 19 & 200 & 112 & 8 & 22 806 & 598 \\
Jester & 15 & 100 & 56 & 18 505 & 73 410 & 41 362 \\
\bottomrule
\end{tabular}
\caption[Ratings distribution]{Summary of the distribution of ratings in each dataset}
\end{table}

\subsubsection{Ratings per user}
All three MovieLens datasets have a similar right-tailed distribution of the number of ratings made per user. The majority of users have made a small number of ratings -- between 1 and 100 --, with a small number of users having rated a very high number of movies.

The Goodbooks-10k dataset user ratings follow a Gaussian distribution, with the majority of users having rated between 75 and 130 books. No user has rated fewer than 19, or more than 200, books.

The Jester dataset has a fairly uniform distribution that is skewed to the right, with two distinct groups of users who have rated between 75 and 80 and between 95 and 100 respectively.

Figures \ref{fig:ML10M-users}, \ref{fig:goodbooks-users} and \ref{fig:jester-users} show the distributions across the users of the ML 10M, Goodbooks-10k and Jester datasets. MovieLens 1M and 100k have similar distributions, albeit each with fewer users than the ML 10M shown below.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Figures/3_ratings-distributions/ml_10m_user-ratings.pdf}
\caption{ML 10M distribution of user ratings}
\label{fig:ML10M-users}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Figures/3_ratings-distributions/goodbooks_user-ratings.pdf}
\caption{Goodbooks-10k distribution of user ratings}
\label{fig:goodbooks-users}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Figures/3_ratings-distributions/jester_user-ratings.pdf}
\caption{Jester distribution of user ratings}
\label{fig:jester-users}
\end{figure}

\subsubsection{Ratings per item}
The MovieLens datasets show a similar right-tailed distribution in terms of the number of ratings made per movie; however, since there are fewer movies than users, movies have a higher average number of ratings.

The books in the Goodbooks-10k dataset do not show the same normal distribution as is the case with ratings per user. Instead, this dataset shows a similar patter to the MovieLens datasets.

Since there are only 100 jokes in the Jester dataset, each joke receives a far greater number of ratings on average than the items in the other datasets. Each joke has been rated a minimum of 18 505 times, while the most ratings received by one joke is 73 410.

Figures \ref{fig:ML10M-items}, \ref{fig:goodbooks-items} and \ref{fig:jester-items} show the distributions of ratings across the items of the ML 10M, Goodbooks-10k and Jester datasets. As is the case with user distributions, the three MovieLens datasets all have similar distributions across item ratings and so only the largest of the datasets is shown below.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Figures/3_ratings-distributions/ml_10m_movie-ratings.pdf}
\caption{ML 10M distribution of movie ratings}
\label{fig:ML10M-items}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Figures/3_ratings-distributions/goodbooks-ratings.pdf}
\caption{Goodbooks-10k distribution of book ratings}
\label{fig:goodbooks-items}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Figures/3_ratings-distributions/jester_joke-ratings.pdf}
\caption{Jester distribution of joke ratings}
\label{fig:jester-items}
\end{figure}

\subsubsection{Rating scale}
In all of the datasets, the data are skewed in favour of positive ratings. For example, MovieLens 100k and 1m , as well as Goodbooks-10k, all use a 5-point integer-only rating scale. The middle value of this scale is 3 stars out of 5; however in all three, the average rating is above this value. Table \ref{tab:ratings-5-number-summaries} shows the 5-number summaries of each dataset.

\begin{table}[H]
\label{tab:ratings-5-number-summaries}
\centering
\begin{tabular}{c | c | c | c | c | c | c}
\toprule
\textbf{Name} & \textbf{Min rating} & \textbf{Q2} & \textbf{Median} & \textbf{Q4} & \textbf{Max rating} & \textbf{Mean rating} \\
\midrule
ML 100K & 1 & 3 & 4 & 4 & 5 & 3.53 \\
ML 1M & 1 & 3 & 4 & 4 & 5 & 3.58 \\
ML 10M & 0.5 & 3.0 & 4.0 & 4.0 & 5.0 & 3.51 \\
Goodbooks-10k & 1 & 3 & 4 & 5 & 5 & 3.92 \\
Jester & -10.0 & -3.25 & 1.36 & 5.05 & 10.0 & 0.74 \\
\bottomrule
\end{tabular}
\caption[5-number of summaries of ratings]{Summary of the distribution of ratings in each dataset}
\end{table}

As seen in table \ref{tab:ratings-5-number-summaries}, in each dataset, both the median and mean rating value are higher than the middle-most value in the respective rating scale.

\subsection{Distribution of genres}
All MovieLens datasets, as well as Goodbooks-10k, include item metadata. In the case of MovieLens, movies have been tagged with at least one of a total of 18 genres. Goodbooks-10k books have been tagged with at least one of 10 genres.

Table \ref{tab:ML-genres} shows the 18 genre categories used in the MovieLens datasets, as well as the proportion of movies that were assigned that genre.

\begin{table}[H]
\label{tab:ML-genres}
\centering
\begin{tabular}{c | c | c | c}
\toprule
\textbf{Genre} & \textbf{ML 100k} & \textbf{ML 1M} & \textbf{ML 10M} \\
\midrule
Action & 0.149 & 0.134 & 0.138 \\
Adventure & 0.080 & 0.076 & 0.096 \\
Animation & 0.025 & 0.028 & 0.027 \\
Children's & 0.073 & 0.067 & 0.049 \\
Comedy & 0.300 & 0.314 & 0.350 \\
Crime & 0.065 & 0.054 & 0.105 \\
Documentary & 0.030 & 0.030 & 0.045 \\
Drama & 0.431 & 0.403 & 0.500 \\
Fantasy & 0.013 & 0.018 & 0.051 \\
Film-Noir & 0.014 & 0.012 & 0.014 \\
Horror & 0.054 & 0.091 & 0.095 \\
Musical & 0.033 & 0.030 & 0.041 \\
Mystery & 0.036 & 0.028 & 0.048 \\
Romance & 0.147 & 0.124 & 0.158 \\
Sci-Fi & 0.060 & 0.074 & 0.071 \\
Thriller & 0.149 & 0.131 & 0.160 \\
War & 0.042 & 0.038 & 0.048 \\
Western & 0.016 & 0.018 & 0.026 \\
\bottomrule
\end{tabular}
\caption[Movies per genre as a proportion of the total]{Summary of the distribution of ratings in each dataset}
\end{table}

\section{Model architecture}
The process of predicting genres from explicit user-item interactions requires two steps. First, train a latent factor CF model to predict ratings. Then capture the item latent factors learnt by the model and use them to train a second model capable of mapping item latent factors to genres.

\section{Ratings prediction}

\subsection{Embedding layers}

\subsection{Model architecture}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{Figures/3_ratings-model.png}
\decoRule
\caption[Ratings prediction model]{Rating prediction model uses embeddings to capture latent factors of items and users}
\label{fig:3_rating-prediction-architecture}
\end{figure}

\subsection{Hyper parameters}

\section{Genre prediction}

\subsection{Model architecture}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{Figures/3_genre-model.png}
\decoRule
\caption[Genre prediction model]{Genre prediction model re-uses the item embedding layer from the base ratings prediction model}
\label{fig:3_genre-prediction-architecture}
\end{figure}

\subsection{Hyper parameters}